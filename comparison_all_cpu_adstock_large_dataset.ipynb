{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e16e3e-14c4-4442-b371-d47c9884b7a8",
   "metadata": {},
   "source": [
    "# A Google Meridian PyMC-Marketing Comparison\n",
    "\n",
    "## Enterprise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b496155-686b-44c9-a23b-750af08a3fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # visible in this process + all children\n",
    "os.environ['JAX_PLATFORMS']='cpu'\n",
    "\n",
    "# Standard imports\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# IPython and Jupyter-specific\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# Third-party imports\n",
    "import arviz as az\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "from pympler import asizeof\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "import nutpie\n",
    "\n",
    "# PyMC Marketing\n",
    "from pymc_marketing.mmm import GeometricAdstock, LogisticSaturation, HillSaturationSigmoid\n",
    "from pymc_marketing.mmm.multidimensional import (\n",
    "    MMM,\n",
    "    MultiDimensionalBudgetOptimizerWrapper,\n",
    ")\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "# Meridian\n",
    "from meridian import constants\n",
    "from meridian.analysis import analyzer, formatter, optimizer, summarizer, visualizer\n",
    "from meridian.data import (\n",
    "    data_frame_input_data_builder,\n",
    "    input_data,\n",
    "    load,\n",
    "    test_utils,\n",
    ")\n",
    "from meridian.model import model, prior_distribution, spec\n",
    "\n",
    "# Data generation\n",
    "from mmm_param_recovery.data_generator import (\n",
    "    generate_mmm_dataset,\n",
    "    get_preset_config,\n",
    ")\n",
    "\n",
    "\n",
    "# Warnings and plotting style\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 7]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef5211-d3f3-4057-8b73-59c6a06042a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproduceability\n",
    "seed: int = sum(map(ord, \"mmm_multidimensional\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
    "tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655f1bb-44fd-49fc-9fe6-7264efd1009b",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b555d-5d8d-41b8-8a37-f76767a382b5",
   "metadata": {},
   "source": [
    "A description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06838a44-9391-46e2-aeb7-717b36d39241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names= [\"large_business\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b45864-a84f-4d23-b36c-0978ae81a760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "channel_columns_list = []\n",
    "truth_dfs = []\n",
    "\n",
    "for name in dataset_names:\n",
    "    business_config = get_preset_config(name.split(\"-\",1)[0])\n",
    "    memo = business_result = generate_mmm_dataset(business_config)\n",
    "    data_df = business_result['data'].rename(columns = {\"date\": \"time\"})\n",
    "    print(f\"{name.replace('_', ' ').title()} dataset shape: {data_df.shape}\")\n",
    "    data_df[\"population\"] = 1\n",
    "    print(f\"Regions: {data_df['geo'].unique()}\")\n",
    "    print(f\"Date range: {data_df['time'].min()} to {data_df['time'].max()}\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*20)\n",
    "    print(\"\")\n",
    "    datasets.append(data_df)\n",
    "    channel_columns_list.append([col for col in data_df.columns if re.match(r\"x\\d+\", col)])\n",
    "    truth_dfs.append(business_result['ground_truth']['transformed_spend'].reset_index().rename(columns={\"date\": \"time\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1b6bf-e6c1-4985-bcab-a87cb2cc6517",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8a6a0-f319-4f7b-816f-52c0fcc00a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Meridian\n",
    "\n",
    "# Storage for built data objects\n",
    "built_data_objects = []\n",
    "\n",
    "for data_df, channel_columns in zip(datasets, channel_columns_list):\n",
    "    # Detect control columns in the form of 'c1', ..., 'cN' if present\n",
    "    potential_controls = ['c' + str(idx) for idx in range(data_df.shape[1])]\n",
    "    control_cols = [col for col in potential_controls if col in data_df.columns]\n",
    "\n",
    "    # Start building the data input\n",
    "    builder = (\n",
    "        data_frame_input_data_builder.DataFrameInputDataBuilder(kpi_type='revenue')\n",
    "        .with_kpi(data_df, kpi_col=\"y\")\n",
    "        .with_population(data_df)\n",
    "    )\n",
    "\n",
    "    # Add controls only if any are found\n",
    "    if control_cols:\n",
    "        builder = builder.with_controls(data_df, control_cols=control_cols)\n",
    "\n",
    "    # Add media columns\n",
    "    builder = builder.with_media(\n",
    "        data_df,\n",
    "        media_cols=channel_columns,\n",
    "        media_spend_cols=channel_columns,\n",
    "        media_channels=channel_columns,\n",
    "    )\n",
    "\n",
    "    # Finalize the builder and store the result\n",
    "    built_data = builder.build()\n",
    "    built_data_objects.append(built_data)\n",
    "\n",
    "# PyMC-Marketing\n",
    "# None required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf497268-712f-438f-816c-cb320f8007aa",
   "metadata": {},
   "source": [
    "## Prior Specification \n",
    "\n",
    "**Meridian**\n",
    "\n",
    "- Uses spend share as prior variance for $\\beta$ parameters, across channels - does not allow for geo level.\n",
    "- Hierarchical structure accross the saturation parameters - Meridian does this by default.\n",
    "- Setting knots to occur every 26 weeks, to best align with seasonality of order 2.\n",
    "\n",
    "**PyMC-Marketing**\n",
    "\n",
    "- Using spend share as prior for $\\beta$ parameters, across channels and geos.\n",
    "- Hierarchical structure accross the saturation parameters.\n",
    "- Includes fourier seasonality of order 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd55e3-80e2-458e-9fb7-9ebe6085cfb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spend_share_list = []\n",
    "prior_sigma_list = []\n",
    "\n",
    "for data_df, channel_columns in zip(datasets, channel_columns_list):\n",
    "    n_channels = len(channel_columns)\n",
    "\n",
    "    # Group and sum media spend by geo\n",
    "    sum_spend_geo_channel = data_df.groupby(\"geo\")[channel_columns].sum()\n",
    "\n",
    "    # Calculate spend share\n",
    "    spend_share = (\n",
    "        sum_spend_geo_channel.to_numpy() /\n",
    "        sum_spend_geo_channel.sum(axis=1).to_numpy()[:, None]\n",
    "    )\n",
    "\n",
    "    # Calculate prior sigma\n",
    "    prior_sigma = n_channels * spend_share\n",
    "\n",
    "    # Store results\n",
    "    prior_sigma_list.append(prior_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51375aa7-33d5-4901-9a63-01647cb5a850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meridian_priors = []\n",
    "pymc_saturation_objects = []\n",
    "\n",
    "for data, channel_columns, prior_sigma in zip(built_data_objects, channel_columns_list, prior_sigma_list):\n",
    "    # Meridian setup\n",
    "    n_time = len(data.time)\n",
    "    knots = np.arange(0, n_time, 26).tolist()\n",
    "\n",
    "    build_media_channel_args = data.get_paid_media_channels_argument_builder()\n",
    "\n",
    "    beta_m = build_media_channel_args(\n",
    "        **{\n",
    "            col: (0, float(prior_sigma.mean(axis=0)[i]))\n",
    "            for i, col in enumerate(channel_columns)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    beta_m_mu, beta_m_sigma = zip(*beta_m)\n",
    "\n",
    "    prior = prior_distribution.PriorDistribution(\n",
    "        beta_m=tfp.distributions.LogNormal(\n",
    "            beta_m_mu, beta_m_sigma, name=constants.BETA_M\n",
    "        )\n",
    "    )\n",
    "    meridian_priors.append(prior)\n",
    "\n",
    "    # PyMC-Marketing setup\n",
    "\n",
    "    saturation = HillSaturationSigmoid(\n",
    "        priors = {\n",
    "            \"sigma\":  Prior(\n",
    "                \"InverseGamma\",\n",
    "                mu=Prior(\"HalfNormal\", sigma=prior_sigma.mean(axis=0), dims = (\"channel\",)),\n",
    "                sigma=Prior(\"HalfNormal\", sigma=1.5),\n",
    "                dims=(\"channel\", \"geo\")),\n",
    "            \"beta\": Prior(\"HalfNormal\", sigma=1.5, dims = (\"channel\",)),\n",
    "            \"lam\":  Prior(\"HalfNormal\", sigma=1.5, dims = (\"channel\",)),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    pymc_saturation_objects.append(saturation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76950195-181a-4e63-b7ea-1bf245b73d95",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9bdde-a40e-4493-8954-d9ca91998007",
   "metadata": {},
   "source": [
    "**Meridian**\n",
    "\n",
    "This is a geo-level model with the following specification:\n",
    "\n",
    "$$\n",
    "y_{g,t} = \\mu_{t} +\\tau_{g} + \\sum_{i=1}^{N_C} \\gamma^{[C]}_{g,i} z_{g,t,i} \n",
    "+ \\sum_{i=1}^{N_M} \\beta^{[M]}_{g,i} \\text{HillAdstock} \\left( \\{ x^{[M]}_{g,t-s,i} \\}_{s=0}^{L} ; \\alpha^{[M]}_i, ec^{[M]}_{i}, slope^{[M]}_{i} \\right) + \\epsilon_{g,t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mu_{t}$ represents the intercept at time $t$.\n",
    "- $\\tau_{g}$ represents the intercept for geo $g$.\n",
    "- $\\sum_{i=1}^{N_C} \\gamma^{[C]}_{g,i} z_{g,t,i}$ represents the control contribution at time $t$ for geo $g$.\n",
    "- $\\sum_{i=1}^{N_M} \\beta^{[M]}_{g,i} \\text{HillAdstock} \\left( \\{ x^{[M]}_{g,t-s,i} \\}_{s=0}^{L} ; \\alpha^{[M]}_i, ec^{[M]}_{i}, slope^{[M]}_{i} \\right)$ represents the adstocked saturated media contribution at time $t$ for geo $g$.\n",
    "- $\\epsilon_{g,t}$ represents the error at time $t$ for geo $g$\n",
    "\n",
    "----\n",
    "\n",
    "**PyMC-Marketing**\n",
    "\n",
    "This is a geo-level model with the following specification:\n",
    "\n",
    "$$\n",
    "y_{g,t} = \\mu_g + \\sum_{i=0}^{4} \\gamma^{[F]}_{g,i} f_{g,t,i} \n",
    "+ \\sum_{i=1}^{N_C} \\gamma^{[C]}_{g,i} z_{g,t,i} \n",
    "+ \\sum_{i=1}^{N_M} \\sigma^{[M]}_{g,i} \\text{HillAdstock} \\left( \\{ x^{[M]}_{g,t-s,i} \\}_{s=0}^{L} ; \\alpha^{[M]}_i, \\lambda^{[M]}_{i}, \\beta^{[M]}_{i}\\right) + \\epsilon_{g,t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mu_g$ represents the intercept for geo g.\n",
    "- $\\sum_{i=0}^{4} \\gamma^{[F]}_{g,i} f_{g,t,i}$ represents the seasonality/fourier contribution (order 2) at time $t$ for geo $g$.\n",
    "- $\\sum_{i=1}^{N_C} \\gamma^{[C]}_{g,i} z_{g,t,i}$ represents the control contribution at time $t$ for geo $g$.\n",
    "- $\\sum_{i=1}^{N_M} \\sigma^{[M]}_{g,i} \\text{HillAdstock} \\left( \\{ x^{[M]}_{g,t-s,i} \\}_{s=0}^{L} ; \\alpha^{[M]}_i, \\lambda^{[M]}_{i}, \\beta^{[M]}_{i}\\right)$ represents the adstocked saturated media contribution at time $t$ for geo $g$.\n",
    "- $\\epsilon_{g,t}$ represents the error at time $t$ for geo $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d2bfb-3475-4ed6-a5d9-d202662a2c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meridian_model_specs = []\n",
    "pymc_mmm_models = []\n",
    "\n",
    "for data_df, data, channel_columns, prior, saturation in zip(\n",
    "    datasets,\n",
    "    built_data_objects,\n",
    "    channel_columns_list,\n",
    "    meridian_priors,\n",
    "    pymc_saturation_objects\n",
    "):\n",
    "    # Detect existing control columns (c1 to cN)\n",
    "    potential_controls = ['c'+ str(i) for i in range(len(data_df.columns))]\n",
    "    control_columns = [col for col in potential_controls if col in data_df.columns]\n",
    "    \n",
    "    # Meridian: model spec\n",
    "    n_time = len(data.time)\n",
    "    knots = np.arange(0, n_time, 26).tolist()\n",
    "\n",
    "    model_spec = spec.ModelSpec(\n",
    "        prior=prior,\n",
    "        media_effects_dist='log_normal',\n",
    "        hill_before_adstock=False,\n",
    "        max_lag=8,\n",
    "        unique_sigma_for_each_geo=True,\n",
    "        roi_calibration_period=None,\n",
    "        rf_roi_calibration_period=None,\n",
    "        knots=knots,\n",
    "        baseline_geo=None,\n",
    "        holdout_id=None,\n",
    "        control_population_scaling_id=None,\n",
    "        media_prior_type='coefficient',\n",
    "        rf_prior_type='coefficient',\n",
    "    )\n",
    "    meridian_model_specs.append(model_spec)\n",
    "\n",
    "    # PyMC-Marketing: MMM model\n",
    "    mmm = MMM(\n",
    "        date_column=\"time\",\n",
    "        target_column=\"y\",\n",
    "        channel_columns=channel_columns,\n",
    "        control_columns=control_columns,\n",
    "        dims=(\"geo\",),\n",
    "        scaling={\n",
    "            \"channel\": {\"method\": \"max\", \"dims\": ()},\n",
    "            \"target\": {\"method\": \"max\", \"dims\": ()},\n",
    "        },\n",
    "        saturation=saturation,\n",
    "        adstock=GeometricAdstock(\n",
    "            l_max=8,\n",
    "            priors={\"alpha\": Prior(\"Beta\", alpha=1, beta=3, dims=(\"channel\",))},\n",
    "        ),\n",
    "        yearly_seasonality=2,\n",
    "    )\n",
    "    pymc_mmm_models.append(mmm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e3ad5-5225-4ec1-8ce4-01a0f37131c4",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e1dd1-ae45-4f06-b210-d86960d97bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meridian_models = []\n",
    "pymc_built_models = []\n",
    "\n",
    "for data_df, data, model_spec, mmm in zip(\n",
    "    datasets,\n",
    "    built_data_objects,\n",
    "    meridian_model_specs,\n",
    "    pymc_mmm_models\n",
    "):\n",
    "    # --- Meridian ---\n",
    "    meridian_instance = model.Meridian(input_data=data, model_spec=model_spec)\n",
    "    meridian_models.append(meridian_instance)\n",
    "\n",
    "    # --- PyMC-Marketing ---\n",
    "    x_train = data_df.drop(columns=[\"y\"])\n",
    "    y_train = data_df[\"y\"]\n",
    "\n",
    "    mmm.build_model(X=x_train, y=y_train)\n",
    "\n",
    "    # Base contribution variables\n",
    "    contribution_vars = [\n",
    "        \"channel_contribution\",\n",
    "        \"intercept_contribution\",\n",
    "        \"yearly_seasonality_contribution\",\n",
    "        \"y\",\n",
    "    ]\n",
    "\n",
    "    # Add control contribution only if controls exist\n",
    "    if any(col in data_df.columns for col in [\"c1\", \"c2\", \"c3\", \"c4\"]):\n",
    "        contribution_vars.insert(1, \"control_contribution\")\n",
    "\n",
    "    mmm.add_original_scale_contribution_variable(var=contribution_vars)\n",
    "\n",
    "    pymc_built_models.append(mmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b727a-6f9d-4d6c-ada4-ba740935f49f",
   "metadata": {},
   "source": [
    "## Prior Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56747471-dc5f-485f-85a6-2bc01fde66ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meridian_prior_samples = []\n",
    "pymc_prior_predictives = []\n",
    "pymc_scalers = []\n",
    "\n",
    "for meridian_model, mmm_model, data_df in zip(\n",
    "    meridian_models,\n",
    "    pymc_built_models,\n",
    "    datasets\n",
    "):\n",
    "    # --- Meridian ---\n",
    "    meridian_model.sample_prior(1000)\n",
    "    meridian_prior_samples.append(meridian_model)\n",
    "\n",
    "    # --- PyMC-Marketing ---\n",
    "    x_train = data_df.drop(columns=[\"y\"])\n",
    "    y_train = data_df[\"y\"]\n",
    "\n",
    "    prior_predictive = mmm_model.sample_prior_predictive(X=x_train, y=y_train, samples=1000)\n",
    "    scalers = mmm_model.get_scales_as_xarray()\n",
    "\n",
    "    pymc_prior_predictives.append(prior_predictive)\n",
    "    pymc_scalers.append(scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32dc23-e72e-4ecd-b8ea-7d156c41dc06",
   "metadata": {},
   "source": [
    "## Posterior Sampling\n",
    "We aim to observe the following:\n",
    "- Speed comparisons\n",
    "- Memory comparisons\n",
    "- Convergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971c254-d605-4fed-a8cc-df480ff8d336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your samplers\n",
    "samplers = [\"pymc\", \"blackjax\", \"numpyro\", \"nutpie\"]\n",
    "\n",
    "# Initialize runtime tracking\n",
    "run_times = {\n",
    "    \"Dataset\": [],\n",
    "    \"Meridian\": [],\n",
    "}\n",
    "\n",
    "esss = {\n",
    "    \"Dataset\": [],\n",
    "    \"Meridian\": [],\n",
    "}\n",
    "for sampler in samplers:\n",
    "    run_times[f\"PyMC-Marketing - {sampler} sampler\"] = []\n",
    "    esss[f\"PyMC-Marketing - {sampler} sampler\"] = []\n",
    "\n",
    "# Loop through datasets and models\n",
    "for i, (meridian_model, mmm_model, data_df) in enumerate(zip(\n",
    "    meridian_models,\n",
    "    pymc_built_models,\n",
    "    datasets\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i-1]\n",
    "    print(f\"\\n--- Dataset {i} ({dataset_name}) ---\")\n",
    "    run_times[\"Dataset\"].append(dataset_name)\n",
    "    esss[\"Dataset\"] += [dataset_name] * 4\n",
    "\n",
    "    # --- Meridian Posterior Sampling ---\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        meridian_model.sample_posterior(\n",
    "            n_chains=4, \n",
    "            n_adapt=500, \n",
    "            n_burnin=500, \n",
    "            n_keep=1000, \n",
    "            seed=(seed, seed),\n",
    "            dual_averaging_kwargs = {\"target_accept_prob\": 0.9},\n",
    "        )\n",
    "        \n",
    "        meridian_time = time.perf_counter() - start\n",
    "        print(f\"Meridian sampling took {meridian_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        meridian_time = None\n",
    "        print(f\"Meridian sampling failed: {e}\")\n",
    "    es = az.ess(meridian_model.inference_data)\n",
    "    min_ess = min([es[val].min() for val in es.data_vars])\n",
    "    es = np.quantile(np.concat([es[var].values.flatten() for var in es.data_vars]),[0.1,0.5,0.9])\n",
    "    run_times[\"Meridian\"].append(meridian_time)\n",
    "    esss[\"Meridian\"] += [np.float64(min_ess)] + list(es)\n",
    "\n",
    "    # Prepare data\n",
    "    x_train = data_df.drop(columns=[\"y\"])\n",
    "    y_train = data_df[\"y\"]\n",
    "\n",
    "    # --- PyMC-Marketing Sampling for each sampler ---\n",
    "    for sampler in samplers:\n",
    "        if sampler in [\"blackjax\", \"numpyro\"] and dataset_name == \"large_business\":\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Running PyMC-Marketing with '{sampler}' sampler...\")\n",
    "\n",
    "            sampler_kwargs = {}\n",
    "            if sampler == \"nutpie\":\n",
    "                sampler_kwargs[\"nuts_sampler_kwargs\"] = {\"backend\": \"jax\", \"gradient_backend\": \"jax\"}\n",
    "\n",
    "            # Fit model\n",
    "            start = time.perf_counter()\n",
    "            mmm_model.fit(\n",
    "                X=x_train,\n",
    "                y=y_train,\n",
    "                chains=4,\n",
    "                draws=1000,\n",
    "                tune=1000,\n",
    "                target_accept=0.9,\n",
    "                random_seed=rng,\n",
    "                nuts_sampler=sampler,\n",
    "                **sampler_kwargs\n",
    "            )\n",
    "            pymc_sample_time = time.perf_counter() - start\n",
    "            print(f\"  Sampling took {pymc_sample_time:.2f} seconds.\")\n",
    "\n",
    "            # Posterior predictive sampling\n",
    "            start = time.perf_counter()\n",
    "            mmm_model.sample_posterior_predictive(\n",
    "                X=x_train,\n",
    "                extend_idata=True,\n",
    "                combined=True,\n",
    "                random_seed=rng,\n",
    "            )\n",
    "            pymc_pp_time = time.perf_counter() - start\n",
    "            print(f\"  Posterior predictive took {pymc_pp_time:.2f} seconds.\")\n",
    "\n",
    "            total_time = pymc_sample_time + pymc_pp_time\n",
    "            es = az.ess(mmm_model.idata, var_names = [var.name for var in mmm_model.model.free_RVs])\n",
    "            min_ess = min([es[val].min() for val in es.data_vars])\n",
    "            es = np.quantile(np.concat([es[var].values.flatten() for var in es.data_vars]),[0.1,0.5,0.9])\n",
    "\n",
    "        except Exception as e:\n",
    "            total_time = None\n",
    "            print(f\"  PyMC-Marketing with '{sampler}' failed: {e}\")\n",
    "\n",
    "        run_times[f\"PyMC-Marketing - {sampler} sampler\"].append(total_time)\n",
    "        esss[f\"PyMC-Marketing - {sampler} sampler\"] += [np.float64(min_ess)] + list(es)\n",
    "        \n",
    "# Create final DataFrame\n",
    "runtime_df = pd.DataFrame(run_times)\n",
    "runtime_df.set_index(\"Dataset\", inplace=True)\n",
    "\n",
    "ess_df = pd.DataFrame(esss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee2b55-ec42-495e-9dc4-bb45f91a5883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(runtime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd7da6-f1a4-495a-8d52-0f3f9addba87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = ess_df.melt(id_vars = \"Dataset\")\n",
    "df[\"ESS\"] = [\"min\", \"5%\", \"50%\", \"95%\"] * 5 * len(dataset_names)\n",
    "df.rename(columns={\"variable\":\"Sampler\"}).pivot(index = [\"Dataset\", \"Sampler\"], columns=\"ESS\", values = \"value\").sort_values(by=\"min\", ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d023d-bba8-44f4-8ea6-150d623fc91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (ess_df.set_index(\"Dataset\") / runtime_df).reset_index().melt(id_vars = \"Dataset\")\n",
    "df[\"ESS / s\"] = [\"min\", \"5%\", \"50%\", \"95%\"] * 5 * len(dataset_names)\n",
    "df.rename(columns={\"variable\":\"Sampler\"}).pivot(index = [\"Dataset\", \"Sampler\"], columns=\"ESS / s\", values = \"value\").sort_values(by=\"min\", ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c76dfb-d71f-46ab-a3de-7b5ca8799153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Note** PyMC-Marketing stores the data and contributions as a part of the model object, meridian does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b475c-d45a-49e0-9603-b4d4a0c1bcd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (meridian_model, mmm_model) in enumerate(zip(\n",
    "    meridian_models,\n",
    "    pymc_built_models\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Dataset {i} ({dataset_name}) ---\")\n",
    "\n",
    "    # Meridian model size\n",
    "    meridian_size = asizeof.asizeof(meridian_model)\n",
    "    print(f\"Meridian model size: {meridian_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    # PyMC-Marketing model size\n",
    "    pymc_size = asizeof.asizeof(mmm_model)\n",
    "    print(f\"PyMC-Marketing model size: {pymc_size / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9a226-b0f0-4fde-a972-70c9b17959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (meridian_model, mmm_model) in enumerate(zip(\n",
    "    meridian_models,\n",
    "    pymc_built_models\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Dataset {i} ({dataset_name}) ---\")\n",
    "\n",
    "    # --- Meridian Diagnostics ---\n",
    "    divergences = meridian_model.inference_data.sample_stats.diverging.sum().item()\n",
    "    print(f\"Meridian number of divergences: {divergences}\")\n",
    "\n",
    "    meridian_rhat_max = az.summary(\n",
    "        meridian_model.inference_data,\n",
    "        var_names=[\n",
    "            \"alpha_m\", \"beta_gm\", \"beta_m\", \"ec_m\",\n",
    "            \"gamma_c\", \"gamma_gc\", \"sigma\", \"tau_g\",\n",
    "            \"xi_c\", \"knot_values\", \"mu_t\"\n",
    "        ]\n",
    "    )['r_hat'].max()\n",
    "    print(f\"Meridian maximum r_hat: {meridian_rhat_max:.3f}\")\n",
    "\n",
    "    # --- PyMC-Marketing Diagnostics ---\n",
    "    divergences = mmm_model.idata.sample_stats.diverging.sum().item()\n",
    "    print(f\"PyMC-Marketing number of divergences: {divergences}\")\n",
    "\n",
    "    pymc_rhat_max = az.summary(\n",
    "        mmm_model.idata,\n",
    "        var_names=[\n",
    "            \"adstock_alpha\", \"gamma_control\", \"gamma_fourier\",\n",
    "            \"intercept_contribution\", \"saturation_beta\",\n",
    "            \"saturation_lam\", \"saturation_sigma\", \"y_sigma\"\n",
    "        ]\n",
    "    )['r_hat'].max()\n",
    "    print(f\"PyMC-Marketing maximum r_hat: {pymc_rhat_max:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f3f89-6300-454d-930b-9f06077d31a6",
   "metadata": {},
   "source": [
    "## Predictive Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9cf512-6959-41f0-9b7f-57c0b811b9af",
   "metadata": {},
   "source": [
    "#### Meridian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8326d-b406-40c5-9a4f-3fcb774b2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (mmm_model, data_df) in enumerate(zip(meridian_models, datasets), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Plotting Posterior Predictive for Meridian model ({dataset_name}) ---\")\n",
    "    model_fit = visualizer.ModelFit(mmm_model) # 0 = small, 1 = medium and 2 = large\n",
    "    fig = model_fit.plot_model_fit(\n",
    "        n_top_largest_geos=len(data_df[\"geo\"].unique()),\n",
    "        show_geo_level=True,\n",
    "        include_baseline=False,\n",
    "        include_ci=True\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3dfc5-9bba-4f53-92d7-18441eed7401",
   "metadata": {},
   "source": [
    "#### PyMC-Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4364bc-8d5b-4608-9aa2-49936b42c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (mmm_model, data_df) in enumerate(zip(pymc_built_models, datasets), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Plotting Posterior Predictive for PyMC-Marketing model ({dataset_name}) ---\")\n",
    "\n",
    "    geos = mmm_model.model.coords[\"geo\"]\n",
    "    dates = mmm_model.model.coords[\"date\"]\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        ncols=len(geos),\n",
    "        figsize=(6 * len(geos), 5),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    # Ensure axes is iterable\n",
    "    if len(geos) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for j, geo in enumerate(geos):\n",
    "        ax = axes[j]\n",
    "        # 94% HDI\n",
    "        az.plot_hdi(\n",
    "            x=dates,\n",
    "            y=mmm_model.idata[\"posterior_predictive\"].y_original_scale.sel(geo=geo),\n",
    "            color=\"C0\",\n",
    "            smooth=False,\n",
    "            hdi_prob=0.94,\n",
    "            fill_kwargs={\"alpha\": 0.2, \"label\": \"94% HDI\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # 50% HDI\n",
    "        az.plot_hdi(\n",
    "            x=dates,\n",
    "            y=mmm_model.idata[\"posterior_predictive\"].y_original_scale.sel(geo=geo),\n",
    "            color=\"C0\",\n",
    "            smooth=False,\n",
    "            hdi_prob=0.5,\n",
    "            fill_kwargs={\"alpha\": 0.4, \"label\": \"50% HDI\"},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Actual data\n",
    "        sns.lineplot(\n",
    "            data=data_df.query(\"geo == @geo\"),\n",
    "            x=\"time\",\n",
    "            y=\"y\",\n",
    "            color=\"black\",\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.set(title=f\"{geo}\")\n",
    "\n",
    "    fig.suptitle(f\"{dataset_name.title()} Dataset – Posterior Predictive\", fontsize=16, fontweight=\"bold\", y=1.03)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28167019-399e-4794-8a61-48580551c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all rows\n",
    "all_performance_rows = []\n",
    "\n",
    "dataset_order = {name: i for i, name in enumerate(dataset_names)}\n",
    "\n",
    "# --- Meridian ---\n",
    "for i, meridian_model in enumerate(meridian_models, start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    model_analysis = analyzer.Analyzer(meridian_model)\n",
    "    fit_data = model_analysis.expected_vs_actual_data()\n",
    "\n",
    "    for geo in fit_data.geo.values:\n",
    "        geo_label = \"geo_a\" if geo == \"national_geo\" else geo\n",
    "\n",
    "        expected = fit_data['expected'].sel(geo=geo, metric=\"mean\")\n",
    "        actual = fit_data['actual'].sel(geo=geo)\n",
    "\n",
    "        expected_vals = expected.values\n",
    "        actual_vals = actual.values\n",
    "\n",
    "        mask = ~np.isnan(expected_vals) & ~np.isnan(actual_vals)\n",
    "        expected_vals = expected_vals[mask]\n",
    "        actual_vals = actual_vals[mask]\n",
    "\n",
    "        if len(expected_vals) < 3:\n",
    "            continue\n",
    "\n",
    "        ss_res = np.sum((actual_vals - expected_vals) ** 2)\n",
    "        ss_tot = np.sum((actual_vals - np.mean(actual_vals)) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
    "\n",
    "        nonzero_mask = actual_vals != 0\n",
    "        mape = (np.abs((actual_vals[nonzero_mask] - expected_vals[nonzero_mask]) / actual_vals[nonzero_mask])).mean() * 100\n",
    "\n",
    "        residuals = actual_vals - expected_vals\n",
    "        dw = durbin_watson(residuals)\n",
    "\n",
    "        all_performance_rows.append({\n",
    "            \"Library\": \"Meridian\",\n",
    "            \"Geo\": geo_label,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"R²\": round(r2, 4) if not np.isnan(r2) else None,\n",
    "            \"MAPE (%)\": round(mape, 2),\n",
    "            \"Durbin-Watson\": round(dw, 3)\n",
    "        })\n",
    "\n",
    "# --- PyMC-Marketing ---\n",
    "for i, (mmm_model, data_df) in enumerate(zip(pymc_built_models, datasets), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "\n",
    "    for geo in mmm_model.model.coords[\"geo\"]:\n",
    "        geo_label = \"geo_a\" if geo == \"Local\" else geo\n",
    "\n",
    "        expected = mmm_model.idata[\"posterior_predictive\"].y_original_scale.mean(['chain', 'draw']).sel(geo=geo)\n",
    "        actual_vals = data_df.loc[data_df[\"geo\"] == geo, \"y\"]\n",
    "\n",
    "        expected_vals = expected.values\n",
    "        actual_vals = actual_vals.values\n",
    "\n",
    "        mask = ~np.isnan(expected_vals) & ~np.isnan(actual_vals)\n",
    "        expected_vals = expected_vals[mask]\n",
    "        actual_vals = actual_vals[mask]\n",
    "\n",
    "        if len(expected_vals) < 3:\n",
    "            continue\n",
    "\n",
    "        ss_res = np.sum((actual_vals - expected_vals) ** 2)\n",
    "        ss_tot = np.sum((actual_vals - np.mean(actual_vals)) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
    "\n",
    "        nonzero_mask = actual_vals != 0\n",
    "        mape = (np.abs((actual_vals[nonzero_mask] - expected_vals[nonzero_mask]) / actual_vals[nonzero_mask])).mean() * 100\n",
    "\n",
    "        residuals = actual_vals - expected_vals\n",
    "        dw = durbin_watson(residuals)\n",
    "\n",
    "        all_performance_rows.append({\n",
    "            \"Library\": \"PyMC-Marketing\",\n",
    "            \"Geo\": geo_label,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"R²\": round(r2, 4) if not np.isnan(r2) else None,\n",
    "            \"MAPE (%)\": round(mape, 2),\n",
    "            \"Durbin-Watson\": round(dw, 3)\n",
    "        })\n",
    "\n",
    "# --- Create and sort final DataFrame ---\n",
    "performance_df = pd.DataFrame(all_performance_rows)\n",
    "\n",
    "# Sort by dataset order and geo\n",
    "performance_df[\"dataset_order\"] = performance_df[\"Dataset\"].map(dataset_order)\n",
    "performance_df = performance_df.sort_values(by=[\"dataset_order\", \"Geo\", \"Library\"]).drop(columns=\"dataset_order\")\n",
    "\n",
    "# Melt to long format for metrics\n",
    "melted_df = performance_df.melt(\n",
    "    id_vars=[\"Dataset\", \"Geo\", \"Library\"],\n",
    "    value_vars=[\"R²\", \"MAPE (%)\", \"Durbin-Watson\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Pivot to get columns per library\n",
    "final_df = melted_df.pivot_table(\n",
    "    index=[\"Dataset\", \"Geo\", \"Metric\"],\n",
    "    columns=\"Library\",\n",
    "    values=\"Value\"\n",
    ").reset_index()\n",
    "\n",
    "# Remove the column index name (optional)\n",
    "final_df.columns.name = None\n",
    "\n",
    "# Reorder and keep only relevant columns\n",
    "final_df = final_df[[\"Dataset\", \"Geo\", \"Metric\", \"PyMC-Marketing\", \"Meridian\"]]\n",
    "\n",
    "# Display final clean table (no 'Library' column)\n",
    "print(\"\\n=== Final Model Performance Comparison ===\")\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c90c26-f05f-4faf-aef5-fa6d687a8d57",
   "metadata": {},
   "source": [
    "## Contribution Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99850beb-9397-45ba-87cb-3a98c04bbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_contributions_ds_list = []\n",
    "\n",
    "for i, truth_df in enumerate(truth_dfs):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Processing ground truth for: {dataset_name} ---\")\n",
    "\n",
    "    contrib_df = truth_df.copy()\n",
    "    channel_cols = [col for col in contrib_df.columns if col.startswith(\"contribution\")]\n",
    "\n",
    "    long_contrib_df = contrib_df.melt(\n",
    "        id_vars=[\"time\", \"geo\"],\n",
    "        value_vars=channel_cols,\n",
    "        var_name=\"media_channel\",\n",
    "        value_name=\"contribution\"\n",
    "    )\n",
    "\n",
    "    long_contrib_df[\"media_channel\"] = long_contrib_df[\"media_channel\"]\n",
    "    long_contrib_df[\"time\"] = pd.to_datetime(long_contrib_df[\"time\"])\n",
    "\n",
    "    true_contributions_da = (\n",
    "        long_contrib_df\n",
    "        .set_index([\"time\", \"geo\", \"media_channel\"])\n",
    "        .to_xarray()\n",
    "    )[\"contribution\"]\n",
    "\n",
    "    true_contributions_ds = true_contributions_da.to_dataset(name=\"contribution\")\n",
    "    true_contributions_ds_list.append(true_contributions_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c777390-0c79-44c0-a4f0-9f04376aa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_contributions_ds_meridian_list = []\n",
    "predicted_contributions_ds_pymc_marketing_list = []\n",
    "\n",
    "for i, (meridian_model, data, mmm_model) in enumerate(zip(\n",
    "    meridian_models,\n",
    "    built_data_objects,\n",
    "    pymc_built_models\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Predicting Contributions for Dataset: {dataset_name} ---\")\n",
    "\n",
    "    # --- Meridian: Predict Contributions ---\n",
    "    model_analysis = analyzer.Analyzer(meridian_model)\n",
    "\n",
    "    incremental_df = model_analysis.incremental_outcome(\n",
    "        aggregate_times=False,\n",
    "        aggregate_geos=False,\n",
    "        use_kpi=True,\n",
    "    )\n",
    "\n",
    "    mean_np = tf.reduce_mean(incremental_df, axis=[0, 1]).numpy()\n",
    "    lower_np = tfp.stats.percentile(incremental_df, 3, axis=[0, 1]).numpy()\n",
    "    upper_np = tfp.stats.percentile(incremental_df, 97, axis=[0, 1]).numpy()\n",
    "\n",
    "    stacked = tf.stack([mean_np, lower_np, upper_np], axis=0).numpy()\n",
    "\n",
    "    contrib_da = xr.DataArray(\n",
    "        stacked,\n",
    "        dims=[\"hdi\", \"geo\", \"time\", \"media_channel\"],\n",
    "        coords={\n",
    "            \"hdi\": [\"mean\", \"lower\", \"upper\"],\n",
    "            \"geo\": data.geo.coords[\"geo\"].values,\n",
    "            \"time\": data.time.coords[\"time\"].values,\n",
    "            \"media_channel\": data.media.coords[\"media_channel\"].values\n",
    "        },\n",
    "        name=\"contribution\"\n",
    "    )\n",
    "\n",
    "    predicted_contributions_ds_meridian = contrib_da.to_dataset()\n",
    "    predicted_contributions_ds_meridian_list.append(predicted_contributions_ds_meridian)\n",
    "\n",
    "    # --- PyMC-Marketing: Predict Contributions ---\n",
    "    contribution_da = mmm_model.idata[\"posterior\"][\"channel_contribution_original_scale\"]\n",
    "    predicted_contributions_ds_pymc_marketing_list.append(contribution_da)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4fd1b-7352-4215-971d-156e03472632",
   "metadata": {},
   "source": [
    "#### Meridian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c28d30-70c7-4638-a039-c9cbe42b7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contributions_grid_with_truth(predicted_contributions_ds, true_contributions_ds):\n",
    "    \"\"\"\n",
    "    Plots a grid of time series with rows = media_channel and cols = geo,\n",
    "    shading between HDI bounds (lower and upper), and overlays true contributions.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_contributions_ds (xarray.Dataset): Posterior Dataset with dims (hdi, geo, time, media_channel)\n",
    "                                                     and coords hdi=[\"mean\", \"lower\", \"upper\"].\n",
    "        true_contributions_ds (xarray.Dataset): True contributions Dataset with dims\n",
    "                                                (time, geo, media_channel) and variable \"contribution\".\n",
    "    \"\"\"\n",
    "    da = predicted_contributions_ds[\"contribution\"]\n",
    "\n",
    "    geos = da.geo.values\n",
    "    channels = da.media_channel.values\n",
    "    time = pd.to_datetime(da.time.values)\n",
    "\n",
    "    n_rows = len(channels)\n",
    "    n_cols = len(geos)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows), sharex=False, sharey=False)\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif n_rows == 1:\n",
    "        axs = np.array([axs])\n",
    "    elif n_cols == 1:\n",
    "        axs = np.array([[ax] for ax in axs])\n",
    "\n",
    "\n",
    "    for i, channel in enumerate(channels):\n",
    "        for j, geo in enumerate(geos):\n",
    "            ax = axs[i, j]\n",
    "\n",
    "            # Posterior mean + HDI\n",
    "            mean = da.sel(hdi=\"mean\", geo=geo, media_channel=channel)\n",
    "            lower = da.sel(hdi=\"lower\", geo=geo, media_channel=channel)\n",
    "            upper = da.sel(hdi=\"upper\", geo=geo, media_channel=channel)\n",
    "\n",
    "            ax.plot(time, mean, label=\"Mean\", linewidth=2)\n",
    "            ax.fill_between(time, lower, upper, alpha=0.3, label=\"HDI (3%–97%)\")\n",
    "\n",
    "            # True contribution\n",
    "            true_channel = f\"contribution_{channel}\"\n",
    "            #try:\n",
    "            truth = true_contributions_ds['contribution'].sel(media_channel = true_channel, geo= \"geo_a\" if geo  == \"national_geo\" else geo)\n",
    "            truth = truth.sel(time=time)\n",
    "            ax.plot(time, truth, linestyle=\"--\", color=\"black\", label=\"True\", linewidth=1.5)\n",
    "            #except KeyError:\n",
    "            #    pass  # Skip if not found\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"Geo: {geo}\", fontsize=12)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f\"{channel}\\nContribution\", fontsize=10)\n",
    "\n",
    "            ax.grid(True)\n",
    "            ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "    for ax in axs[-1]:\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    fig.suptitle(\"Media Channel Contributions: Posterior vs. True\", fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "for i, (predicted_contributions_ds, true_contributions_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_meridian_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Plotting Contributions Grid with Truth ({dataset_name}) ---\")\n",
    "\n",
    "    true = true_contributions_ds.copy(deep=True)\n",
    "\n",
    "    # Rename geo in the true dataset for small business (index 0)\n",
    "    if i == 1 and \"Local\" in true.geo.values:\n",
    "        true = true.assign_coords(geo=[\"geo_a\"] if len(true.geo) == 1 else [\n",
    "            \"geo_a\" if g == \"Local\" else g for g in true.geo.values\n",
    "        ])\n",
    "\n",
    "    plot_contributions_grid_with_truth(predicted_contributions_ds, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b792ec4-e790-4998-a8d1-35b25ff492a1",
   "metadata": {},
   "source": [
    "#### PyMC-Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe90300-7c35-4ef4-b698-418036a5ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contributions_grid_with_truth(predicted_contributions_ds, true_contributions_ds):\n",
    "    \"\"\"\n",
    "    Plots a grid of time series with rows = media_channel and cols = geo,\n",
    "    shading between HDI bounds (3rd–97th percentiles), and overlays true contributions.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_contributions_ds (xarray.Dataset): Dataset with dims (chain, draw, date, geo, channel)\n",
    "        true_contributions_ds (xarray.Dataset): Dataset with vars like 'contribution_x1-seasonal-1'\n",
    "                                                and dims (date, geo)\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    da = predicted_contributions_ds\n",
    "\n",
    "    # Collapse (chain, draw) into posterior samples axis\n",
    "    stacked = da.stack(sample=(\"chain\", \"draw\"))\n",
    "\n",
    "    # Compute posterior mean and HDI bounds\n",
    "    mean = stacked.mean(dim=\"sample\")\n",
    "    lower = stacked.quantile(0.03, dim=\"sample\")\n",
    "    upper = stacked.quantile(0.97, dim=\"sample\")\n",
    "\n",
    "    geos = da.coords[\"geo\"].values\n",
    "    channels = da.coords[\"channel\"].values\n",
    "    dates = pd.to_datetime(da.coords[\"date\"].values)\n",
    "\n",
    "    n_rows = len(channels)\n",
    "    n_cols = len(geos)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows), sharex=False, sharey=False)\n",
    "\n",
    "    # Normalize axs to always be 2D\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif n_rows == 1:\n",
    "        axs = np.array([axs])\n",
    "    elif n_cols == 1:\n",
    "        axs = np.array([[ax] for ax in axs])\n",
    "\n",
    "    for i, channel in enumerate(channels):\n",
    "        for j, geo in enumerate(geos):\n",
    "            ax = axs[i, j]\n",
    "\n",
    "            mean_ts = mean.sel(geo=geo, channel=channel)\n",
    "            lower_ts = lower.sel(geo=geo, channel=channel)\n",
    "            upper_ts = upper.sel(geo=geo, channel=channel)\n",
    "\n",
    "            ax.plot(dates, mean_ts, label=\"Mean\", linewidth=2)\n",
    "            ax.fill_between(dates, lower_ts, upper_ts, alpha=0.3, label=\"HDI (3%–97%)\")\n",
    "\n",
    "            # Plot true contributions\n",
    "            true_var = f\"contribution_{channel}\"\n",
    "            try:\n",
    "                truth = true_contributions_ds['contribution'].sel(media_channel = true_var, geo=geo)\n",
    "                truth = truth.sel(time=dates)\n",
    "                ax.plot(dates, truth, linestyle=\"--\", color=\"black\", label=\"True\", linewidth=1.5)\n",
    "            except KeyError:\n",
    "                pass  # skip if not found\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"Geo: {geo}\", fontsize=12)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f\"{channel}\\nContribution\", fontsize=10)\n",
    "\n",
    "            ax.grid(True)\n",
    "            ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "    for ax in axs[-1]:\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    fig.suptitle(\"Media Channel Contributions: Posterior vs. True\", fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "for i, (predicted_ds, true_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_pymc_marketing_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    print(f\"\\n--- Plotting Contributions Grid with Truth ({dataset_name}) ---\")\n",
    "\n",
    "    # Deep copy both datasets\n",
    "    pred = predicted_ds.copy(deep=True)\n",
    "    true = true_ds.copy(deep=True)\n",
    "\n",
    "    # ✅ Fix geo name in predicted dataset (not true!) for small dataset\n",
    "    if i == 1 and \"Local\" in pred.geo.values:\n",
    "        pred = pred.rename({'geo': 'geo'})  # ensures coordinate is named\n",
    "        pred = pred.assign_coords(geo=(\"geo\", [\"national geo\"]))\n",
    "\n",
    "    plot_contributions_grid_with_truth(predicted_ds, true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0216e2-97eb-4fb5-87ed-c16669fa4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detailed_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "def compute_smape_rows(predicted_ds, true_ds, lib_type, dataset_name):\n",
    "    results = []\n",
    "\n",
    "    def smape(a, f):\n",
    "        return 100 * np.mean(2 * np.abs(f - a) / (np.abs(f) + np.abs(a) + 1e-8))\n",
    "\n",
    "    if lib_type == \"Meridian\":\n",
    "        pred_da = predicted_ds[\"contribution\"].sel(hdi='mean')\n",
    "        for geo in true_ds.coords[\"geo\"].values:\n",
    "            for channel in pred_da.coords[\"media_channel\"].values:\n",
    "                try:\n",
    "                    pred = pred_da.sel(geo=geo if \"national_geo\" not in pred_da.coords[\"geo\"].values else \"national_geo\", media_channel=channel)\n",
    "                    try:\n",
    "                        true = true_ds['contribution'].sel(media_channel=channel, geo=geo)\n",
    "                    except KeyError:\n",
    "                        true = true_ds['contribution'].sel(media_channel=f\"contribution_{channel}\", geo=geo)\n",
    "\n",
    "                    pred_series = pd.Series(pred.values, index=pd.to_datetime(pred.coords[\"time\"].values))\n",
    "                    true_series = pd.Series(true.values, index=pd.to_datetime(true.coords[\"time\"].values))\n",
    "\n",
    "                    df = pd.DataFrame({\"pred\": pred_series, \"true\": true_series}).dropna()\n",
    "                    df = df[df[\"true\"] != 0]\n",
    "\n",
    "                    if not df.empty:\n",
    "                        smape_val = smape(df[\"true\"], df[\"pred\"])\n",
    "                        results.append({\n",
    "                            \"Library\": lib_type,\n",
    "                            \"Dataset\": dataset_name,\n",
    "                            \"geo\": geo,\n",
    "                            \"media_channel\": channel,\n",
    "                            \"SMAPE (%)\": round(smape_val, 2)\n",
    "                        })\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    elif lib_type == \"PyMC Marketing\":\n",
    "        pred_mean = predicted_ds.mean(dim=[\"chain\", \"draw\"])\n",
    "        for geo in predicted_ds.geo.values:\n",
    "            # normalized_geo = \"National\" if geo in [\"national_geo\", \"Local\"] else geo\n",
    "            for channel in predicted_ds.channel.values:\n",
    "                try:\n",
    "                    pred = pred_mean.sel(geo=geo, channel=channel)\n",
    "                    try:\n",
    "                        true = true_ds['contribution'].sel(media_channel=channel, geo=geo)\n",
    "                    except KeyError:\n",
    "                        true = true_ds['contribution'].sel(media_channel=f\"contribution_{channel}\", geo=geo)\n",
    "\n",
    "                    pred_series = pd.Series(pred.values, index=pd.to_datetime(pred[\"date\"].values))\n",
    "                    true_series = pd.Series(true.values, index=pd.to_datetime(true[\"time\"].values))\n",
    "\n",
    "                    df = pd.DataFrame({\"pred\": pred_series, \"true\": true_series}).dropna()\n",
    "                    df = df[df[\"true\"] != 0]\n",
    "\n",
    "                    if not df.empty:\n",
    "                        smape_val = smape(df[\"true\"], df[\"pred\"])\n",
    "                        results.append({\n",
    "                            \"Library\": lib_type,\n",
    "                            \"Dataset\": dataset_name,\n",
    "                            \"geo\": geo,\n",
    "                            \"media_channel\": channel,\n",
    "                            \"SMAPE (%)\": round(smape_val, 2)\n",
    "                        })\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    return results\n",
    "\n",
    "# Meridian\n",
    "for i, (predicted_ds, true_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_meridian_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    true = true_ds.copy(deep=True)\n",
    "\n",
    "    rows = compute_smape_rows(predicted_ds, true, lib_type=\"Meridian\", dataset_name=dataset_name)\n",
    "    all_detailed_rows.extend(rows)\n",
    "    if rows:\n",
    "        summary_rows.append({\n",
    "            \"Library\": \"Meridian\",\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Average SMAPE\": round(np.mean([r[\"SMAPE (%)\"] for r in rows]), 2)\n",
    "        })\n",
    "\n",
    "# PyMC Marketing\n",
    "for i, (predicted_ds, true_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_pymc_marketing_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1].split(\"_\")[0]\n",
    "    rows = compute_smape_rows(predicted_ds, true_ds, lib_type=\"PyMC Marketing\", dataset_name=dataset_name)\n",
    "    all_detailed_rows.extend(rows)\n",
    "    if rows:\n",
    "        summary_rows.append({\n",
    "            \"Library\": \"PyMC Marketing\",\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Average SMAPE\": round(np.mean([r[\"SMAPE (%)\"] for r in rows]), 2)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrames\n",
    "detailed_df = pd.DataFrame(all_detailed_rows)\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# === Format summary: pivot and clean ===\n",
    "summary_df = summary_df.pivot(\n",
    "    index=\"Dataset\",\n",
    "    columns=\"Library\",\n",
    "    values=\"Average SMAPE\"\n",
    ").reset_index()\n",
    "summary_df.columns.name = None\n",
    "summary_df = summary_df.round(2)\n",
    "\n",
    "# === Format detailed: pivot and clean ===\n",
    "detailed_df = detailed_df.pivot_table(\n",
    "    index=[\"Dataset\", \"geo\", \"media_channel\"],\n",
    "    columns=\"Library\",\n",
    "    values=\"SMAPE (%)\"\n",
    ").reset_index()\n",
    "detailed_df.columns.name = None\n",
    "detailed_df = detailed_df.round(2)\n",
    "\n",
    "# Display\n",
    "print(\"\\n=== Summary: Average SMAPE per Dataset ===\")\n",
    "display(summary_df)\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "print(\"\\n=== Detailed SMAPE by Dataset, Geo, and Channel ===\")\n",
    "display(detailed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4cc78-f2bf-4601-ade0-75fdd818dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mape_rows = []\n",
    "mape_summary_rows = []\n",
    "\n",
    "def compute_mape_rows(predicted_ds, true_ds, lib_type, dataset_name):\n",
    "    results = []\n",
    "\n",
    "    def mape(a, f):\n",
    "        return 100 * np.mean(np.abs((a - f) / (a + 1e-8)))  # Avoid divide-by-zero\n",
    "\n",
    "    if lib_type == \"Meridian\":\n",
    "        pred_da = predicted_ds[\"contribution\"].sel(hdi='mean')\n",
    "        for geo in true_ds.coords[\"geo\"].values:\n",
    "            for channel in pred_da.coords[\"media_channel\"].values:\n",
    "                try:\n",
    "                    pred = pred_da.sel(geo=geo if \"national_geo\" not in pred_da.coords[\"geo\"].values else \"national_geo\", media_channel=channel)\n",
    "                    try:\n",
    "                        true = true_ds['contribution'].sel(media_channel=channel, geo=geo)\n",
    "                    except KeyError:\n",
    "                        true = true_ds['contribution'].sel(media_channel=f\"contribution_{channel}\", geo=geo)\n",
    "\n",
    "                    pred_series = pd.Series(pred.values, index=pd.to_datetime(pred.coords[\"time\"].values))\n",
    "                    true_series = pd.Series(true.values, index=pd.to_datetime(true.coords[\"time\"].values))\n",
    "\n",
    "                    df = pd.DataFrame({\"pred\": pred_series, \"true\": true_series}).dropna()\n",
    "                    df = df[df[\"true\"] != 0]\n",
    "\n",
    "                    if not df.empty:\n",
    "                        mape_val = mape(df[\"true\"], df[\"pred\"])\n",
    "                        results.append({\n",
    "                            \"Library\": lib_type,\n",
    "                            \"Dataset\": dataset_name,\n",
    "                            \"geo\": geo,\n",
    "                            \"media_channel\": channel,\n",
    "                            \"MAPE (%)\": round(mape_val, 2)\n",
    "                        })\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    elif lib_type == \"PyMC Marketing\":\n",
    "        pred_mean = predicted_ds.mean(dim=[\"chain\", \"draw\"])\n",
    "        for geo in predicted_ds.geo.values:\n",
    "            for channel in predicted_ds.channel.values:\n",
    "                try:\n",
    "                    pred = pred_mean.sel(geo=geo, channel=channel)\n",
    "                    try:\n",
    "                        true = true_ds['contribution'].sel(media_channel=channel, geo=geo)\n",
    "                    except KeyError:\n",
    "                        true = true_ds['contribution'].sel(media_channel=f\"contribution_{channel}\", geo=geo)\n",
    "\n",
    "                    pred_series = pd.Series(pred.values, index=pd.to_datetime(pred[\"date\"].values))\n",
    "                    true_series = pd.Series(true.values, index=pd.to_datetime(true[\"time\"].values))\n",
    "\n",
    "                    df = pd.DataFrame({\"pred\": pred_series, \"true\": true_series}).dropna()\n",
    "                    df = df[df[\"true\"] != 0]\n",
    "\n",
    "                    if not df.empty:\n",
    "                        mape_val = mape(df[\"true\"], df[\"pred\"])\n",
    "                        results.append({\n",
    "                            \"Library\": lib_type,\n",
    "                            \"Dataset\": dataset_name,\n",
    "                            \"geo\": geo,\n",
    "                            \"media_channel\": channel,\n",
    "                            \"MAPE (%)\": round(mape_val, 2)\n",
    "                        })\n",
    "\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    return results\n",
    "\n",
    "# Meridian\n",
    "for i, (predicted_ds, true_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_meridian_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1]\n",
    "    pred = predicted_ds.copy(deep=True)\n",
    "    if i == 1 and \"national geo\" in pred.geo.values:\n",
    "        pred = pred.assign_coords(geo=[\"Local\"] if len(true.geo) == 1 else true.geo.values)\n",
    "\n",
    "    rows = compute_mape_rows(pred, true_ds, lib_type=\"Meridian\", dataset_name=dataset_name)\n",
    "    all_mape_rows.extend(rows)\n",
    "    if rows:\n",
    "        mape_summary_rows.append({\n",
    "            \"Library\": \"Meridian\",\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Average MAPE\": round(np.mean([r[\"MAPE (%)\"] for r in rows]), 2)\n",
    "        })\n",
    "\n",
    "# PyMC Marketing\n",
    "for i, (predicted_ds, true_ds) in enumerate(zip(\n",
    "    predicted_contributions_ds_pymc_marketing_list,\n",
    "    true_contributions_ds_list\n",
    "), start=1):\n",
    "    dataset_name = dataset_names[i - 1]\n",
    "    rows = compute_mape_rows(predicted_ds, true_ds, lib_type=\"PyMC Marketing\", dataset_name=dataset_name)\n",
    "    all_mape_rows.extend(rows)\n",
    "    if rows:\n",
    "        mape_summary_rows.append({\n",
    "            \"Library\": \"PyMC Marketing\",\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Average MAPE\": round(np.mean([r[\"MAPE (%)\"] for r in rows]), 2)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrames\n",
    "mape_detailed_df = pd.DataFrame(all_mape_rows)\n",
    "mape_summary_df = pd.DataFrame(mape_summary_rows)\n",
    "\n",
    "# === Format summary: pivot and clean ===\n",
    "mape_summary_df = mape_summary_df.pivot(\n",
    "    index=\"Dataset\",\n",
    "    columns=\"Library\",\n",
    "    values=\"Average MAPE\"\n",
    ").reset_index()\n",
    "mape_summary_df.columns.name = None\n",
    "mape_summary_df = mape_summary_df.round(2)\n",
    "\n",
    "# === Format detailed: pivot and clean ===\n",
    "mape_detailed_df = mape_detailed_df.pivot_table(\n",
    "    index=[\"Dataset\", \"geo\", \"media_channel\"],\n",
    "    columns=\"Library\",\n",
    "    values=\"MAPE (%)\"\n",
    ").reset_index()\n",
    "mape_detailed_df.columns.name = None\n",
    "mape_detailed_df = mape_detailed_df.round(2)\n",
    "\n",
    "# Display\n",
    "print(\"\\n=== Summary: Average MAPE per Dataset ===\")\n",
    "display(mape_summary_df)\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "print(\"\\n=== Detailed MAPE by Dataset, Geo, and Channel ===\")\n",
    "display(mape_detailed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2f71e-7e25-4c58-b647-db29dfc54789",
   "metadata": {},
   "source": [
    "## Bias Measure\n",
    "\n",
    "**Total absolute reconstruction error and scaled root mean squared error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8fca6-decd-4a8b-ab1b-28ff4e7116c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmm_param_recovery.evaluation import compute_error_measure, meridian_to_contribution_xr, pymc_marketing_to_contribution_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57867414-b270-4a9a-926c-6e958ad1e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = []\n",
    "abs_errs = []\n",
    "\n",
    "for idx, (meridian_model, mmm_model, true_ds) in enumerate(zip(meridian_models, pymc_mmm_models, true_contributions_ds_list)):\n",
    "    true_ds = (\n",
    "        true_ds\n",
    "        .rename({\"time\":\"date\", \"media_channel\":\"channel\"})\n",
    "        .assign_coords({\"channel\":lambda x: [xi.split(\"contribution_\",1)[1] for xi in x.coords[\"channel\"].values]})\n",
    "    )\n",
    "    contribution_data = []\n",
    "    contribution_data.append(meridian_to_contribution_xr(meridian_model, true_ds))\n",
    "    contribution_data.append(pymc_marketing_to_contribution_xr(mmm_model.idata.posterior))\n",
    "    contribution_xr = xr.concat(contribution_data, dim=xr.Variable(\"model\", [\"Meridian\", \"PyMC-Marketing\"]))\n",
    "    biases.append(\n",
    "        compute_error_measure(\n",
    "            contribution_xr,\n",
    "            true_ds\n",
    "        ).quantile([0.04, 0.5, 0.96], dim=[\"geo\", \"chain\", \"draw\", \"channel\"]))\n",
    "    abs_errs.append(\n",
    "        np.sqrt(compute_error_measure(\n",
    "            contribution_xr,\n",
    "            true_ds,\n",
    "            error_measure=\"srmse\"\n",
    "        ).mean([\"channel\", \"geo\"]) / true_ds.mean([\"date\", \"channel\", \"geo\"])).quantile([0.04, 0.5, 0.96], dim=[\"chain\", \"draw\"]))\n",
    "biases = xr.concat(biases, xr.Variable(\"dataset\",dataset_names))\n",
    "abs_errs = xr.concat(abs_errs, xr.Variable(\"dataset\",dataset_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090200b4-e2bc-4f2d-b1f3-43cfab7aac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_tables import GT, style, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b8bd0-468f-4235-adb0-8d9416e80e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recon = (biases.to_dataframe().join(abs_errs.to_dataframe(), lsuffix=\"_bias\", rsuffix=\"_abs_error\")\n",
    "    .reset_index()\n",
    "    .pivot(index=[\"dataset\", \"model\"], columns=\"quantile\", values=[\"contribution_bias\", \"contribution_abs_error\"])\n",
    "   .reset_index()\n",
    ")\n",
    "\n",
    "recon.columns = recon.columns.map(lambda x: f'{x[0]}_{x[1]}'.replace(\".\", \"_\"))\n",
    "recon.rename(columns={\"dataset_\":\"dataset\", \"model_\":\"model\"}, inplace=True)\n",
    "recon[\"dataset\"] = recon[\"dataset\"].map(lambda x: x.replace(\"_\", \" \").title()).values\n",
    "\n",
    "GT(recon,\n",
    "   groupname_col=\"dataset\",\n",
    "   rowname_col=\"model\"\n",
    "  ).tab_spanner(\n",
    "    label = \"Avg. Error\", columns=[\"contribution_bias_0_5\", \"contribution_bias_0_04\", \"contribution_bias_0_96\"]\n",
    ").tab_spanner(\n",
    "    label = \"Scaled RMSE\", columns=[\"contribution_abs_error_0_5\", \"contribution_abs_error_0_04\", \"contribution_abs_error_0_96\"]\n",
    ").cols_label(\n",
    "    contribution_bias_0_5=\"Mean\",\n",
    "    contribution_bias_0_04 = \"4%\",\n",
    "    contribution_bias_0_96 = \"96%\",\n",
    "    contribution_abs_error_0_5 = \"Mean\",\n",
    "    contribution_abs_error_0_04 = \"4%\",\n",
    "    contribution_abs_error_0_96 = \"96%\"\n",
    ").fmt_number(\n",
    "    decimals=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
