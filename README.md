# MMM Parameter Recovery

This repository provides a comprehensive benchmarking suite for Marketing Mix Modeling (MMM) parameter recovery studies. It includes both lightweight data generation tools and full benchmarking capabilities for comparing different Bayesian inference methods.

## Package Structure

This repository contains two separate packages:

### ðŸŽ¯ mmm-data-generator
**Lightweight package for synthetic MMM data generation**
- Minimal dependencies (numpy, pandas, matplotlib, seaborn, pymc-marketing)
- Configurable channel patterns, geographic variations, and transformations
- Ground truth parameters for model validation
- Perfect for standalone data generation in other projects

**Installation:**
```bash
pip install mmm-data-generator
```

### ðŸ”¬ mmm-param-recovery  
**Full benchmarking suite for MMM parameter recovery**
- Comprehensive evaluation of PyMC, NumPyro, and Meridian samplers
- Bayesian metrics, convergence diagnostics, and performance benchmarks
- Rich visualizations and automated benchmarking workflows
- Includes all heavy dependencies for complete MMM analysis

**Installation:**
```bash
pip install mmm-param-recovery
```

## Quick Start

### Data Generation Only
```python
from mmm_data_generator import generate_mmm_dataset, get_preset_config

# Generate synthetic MMM data
config = get_preset_config('small_business')
data = generate_mmm_dataset(config=config)
```

### Full Benchmarking
```python
from mmm_param_recovery.benchmarking import run_benchmark
from mmm_data_generator import get_preset_config

# Run comprehensive benchmark
config = get_preset_config('small_business')
results = run_benchmark(
    dataset_config=config,
    samplers=['pymc_nuts', 'numpyro', 'meridian']
)
```

> **Note**  
> A summary of the results of this benchmark has been published in this [blog post](https://www.pymc-labs.com/blog-posts/pymc-marketing-vs-google-meridian).

## Quick Start

### Environment Setup

When working with GCP instances with preinstalled Jupyter Lab,
we have found Conda to be the easiest method.

```shell
conda env create -f cpu-environment.yaml
conda activate python312-cpu
```

### Running Benchmarks

The main benchmarking script provides a flexible CLI for comparing PyMC-Marketing and Meridian across different datasets and configurations.

#### Basic Usage

```bash
# Quick test with small dataset (recommended for laptops)
python run_benchmark.py --datasets small_business --samplers nutpie --chains 2 --draws 500 --tune 500

# Standard benchmark
python run_benchmark.py --datasets small_business medium_business \
                       --samplers pymc blackjax numpyro nutpie \
                       --chains 4 --draws 1000 --tune 1000

# Force re-run even if cached results exist
python run_benchmark.py --datasets small_business --force-rerun

# Use cached results (default behavior)
python run_benchmark.py --datasets small_business --no-force-rerun

# Generate only plots from existing models
python run_benchmark.py --datasets small_business --plots-only
```

#### Available Options

- `--datasets`: Choose from `small_business`, `medium_business`, `large_business`, `growing_business`
- `--samplers`: PyMC samplers - `pymc`, `blackjax`, `numpyro`, `nutpie` 
- `--libraries`: Run only specific libraries - `meridian`, `pymc`
- `--chains`: Number of MCMC chains (default: 4)
- `--draws`: Number of draws per chain (default: 1000)
- `--tune`: Number of tuning steps (default: 1000)
- `--target-accept`: Target acceptance probability (default: 0.9)
- `--seed`: Random seed for reproducibility
- `--force-rerun`: Force re-run even if cached results exist
- `--plots-only`: Generate only plots from existing models

#### Output Structure

```
data/results/
â”œâ”€â”€ {dataset_name}/
â”‚   â”œâ”€â”€ data.pkl                    # Cached generated dataset
â”‚   â”œâ”€â”€ meridian_model.pkl          # Fitted Meridian model
â”‚   â”œâ”€â”€ meridian_stats.pkl          # Meridian runtime & ESS
â”‚   â”œâ”€â”€ pymc_{sampler}_model.nc     # PyMC models per sampler
â”‚   â”œâ”€â”€ pymc_{sampler}_stats.pkl    # PyMC stats per sampler
â”‚   â””â”€â”€ plots/                      
â”‚       â”œâ”€â”€ posterior_predictive_meridian.html
â”‚       â””â”€â”€ posterior_predictive_pymc_{sampler}.png
â””â”€â”€ summary/
    â”œâ”€â”€ runtime_comparison.csv       # Runtime comparison table
    â”œâ”€â”€ ess_comparison.csv          # ESS metrics table
    â”œâ”€â”€ performance_metrics.csv     # RÂ², MAPE, Durbin-Watson
    â””â”€â”€ diagnostics_summary.csv     # Convergence diagnostics
```


## Model Parameterization and Priors

This section details the exact model specifications used in the benchmark to ensure reproducibility.

### PyMC-Marketing MMM

#### Model Structure
- **Adstock**: Geometric adstock with maximum lag of 8 weeks
- **Saturation**: Hill saturation with sigmoid transformation (hierarchical)
- **Seasonality**: Yearly seasonality with 2 Fourier modes
- **Scaling**: Max scaling for both channels and target (dims=())

#### Prior Specifications
```python
# Adstock decay
adstock_alpha ~ Beta(alpha=1, beta=3)  # Per channel

# Saturation parameters (hierarchical)
saturation_sigma ~ InverseGamma(
    mu=HalfNormal(sigma=prior_sigma_per_channel),  # Population mean
    sigma=HalfNormal(sigma=1.5),                    # Population std
    dims=("channel", "geo")
)
saturation_beta ~ HalfNormal(sigma=1.5)  # Per channel
saturation_lam ~ HalfNormal(sigma=1.5)   # Per channel

# Control effects
gamma_control ~ Normal(0, 2)  # Implicit default, per control

# Intercept
intercept ~ Normal(mu=y_mean, sigma=y_std*2)  # Per geo

# Seasonality
gamma_fourier ~ Normal(0, 1)  # Per geo Ã— fourier_mode

# Observation noise
y_sigma ~ HalfNormal(sigma=y_std*2)  # Per geo
```

Where `prior_sigma_per_channel = n_channels * spend_share` (spend share normalized per geo).

### Google Meridian

#### Model Structure
- **Adstock**: Geometric adstock with maximum lag of 8 weeks
- **Saturation**: Hill transformation applied after adstock
- **Trend**: Spline-based with knots every 26 weeks
- **Geo effects**: Unique sigma for each geo

#### Prior Specifications
```python
# Adstock decay  
alpha_m ~ Beta(alpha=1.0, beta=3.0)  # Per channel (matching PyMC)

# Media coefficients
beta_m ~ LogNormal(mu=0, sigma=prior_sigma_per_channel)  # Population level
beta_gm ~ Normal(0, 1)  # Geo-specific deviations
eta_m ~ HalfNormal(1)  # Media noise

# Hill saturation
ec_m ~ Beta(2, 2)  # Half-saturation point
slope_m ~ Gamma(2, 1)  # Hill slope

# Control effects
gamma_c ~ Normal(0, 1)  # Population level
gamma_gc ~ Normal(0, 1)  # Geo-specific
xi_c ~ HalfNormal(1)  # Control noise

# Baseline
tau_g ~ Normal(0, 5)  # Per geo

# Trend splines
knot_values ~ Normal(0, 5)  # Per knot

# Observation noise
sigma ~ HalfNormal(5)  # Single global parameter
```

### Key Differences

1. **Media Response Parameterization**:
   - PyMC: Uses hierarchical Hill saturation parameters that directly encode media effects
   - Meridian: Separates linear coefficients (beta) from saturation transformation (ec, slope)

2. **Geo-level Modeling**:
   - PyMC: Hierarchical priors with partial pooling across geos
   - Meridian: Explicit geo-specific parameters with population baselines

3. **Time-varying Effects**:
   - PyMC: Fourier-based seasonality (2 modes)
   - Meridian: Spline-based trend (knots every 26 weeks)

4. **Prior Informativeness**:
   - Both use weakly informative priors
   - Prior sigma for media effects based on spend shares in both models
   - Adstock priors favor faster decay (E[alpha] = 0.25)

### Parameter Count Comparison

For a dataset with G geos, C channels, and K controls:

| Component | PyMC-Marketing | Meridian |
|-----------|---------------|----------|
| Adstock | C | C |
| Media Response | CÃ—(3 + G) + hierarchical | CÃ—(5 + G) |
| Controls | K | KÃ—(2 + G) + K |
| Baseline/Intercept | G | G |
| Time Effects | GÃ—4 (seasonality) | 4-6 (trend knots) |
| Noise | G | 1 |

## Notes

- **Memory constraints**: `blackjax` and `numpyro` samplers are automatically skipped for large datasets
- **Sequential execution**: Models are fitted one at a time to conserve CPU resources
- **Incremental saving**: Each model is saved immediately after fitting to prevent data loss
- **Caching**: Dataset generation and model fitting results are cached by default

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

Copyright 2025 PyMC Labs
